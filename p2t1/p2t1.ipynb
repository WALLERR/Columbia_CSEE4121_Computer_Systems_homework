{"cells": [{"cell_type": "code", "execution_count": 4, "id": "dc14e444", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/04/30 00:49:42 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n22/04/30 00:49:42 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Stopping existing streaming query [id=7ff3078b-b974-4c12-b723-b9136c5e1aa8, runId=a2db2401-2bcc-4b74-b7f0-66704bc7cfe6], as a new run is being started.\n22/04/30 00:49:42 ERROR org.apache.spark.sql.execution.streaming.MicroBatchExecution: Query [id = 7ff3078b-b974-4c12-b723-b9136c5e1aa8, runId = a2db2401-2bcc-4b74-b7f0-66704bc7cfe6] terminated with error\njava.io.IOException: Failed to get result: java.lang.InterruptedException\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:898)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.listFileInfo(GoogleCloudStorageFileSystem.java:1037)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:856)\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:248)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:301)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:128)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:325)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:385)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:193)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:382)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:613)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:378)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:211)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:191)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFromFuture(GoogleCloudStorageFileSystem.java:892)\n\t... 48 more\n"}, {"data": {"text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7fcd00ee9730>"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "import os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('Streaming').getOrCreate()\n\nfrom pyspark.sql.types import StructType\n\nstruct_type = StructType().add('article', 'string').add('rank', 'double')\n\n\ndf = spark.readStream.option(\"sep\", \"\\t\").schema(struct_type).csv(\"gs://dataproc-staging-us-central1-398592550746-z8akpufv/pagerank_whole\")\n\ndf_5 = df.select('article', 'rank').where('rank > 0.5')\n\ndf_5.writeStream.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n    .option(\"checkpointLocation\", \"hdfs://cluster-zihao-node1-m/user/root/p2t1\") \\\n    .option(\"path\", \"hdfs://cluster-zihao-node1-m/user/root/p2t1\").start()"}, {"cell_type": "code", "execution_count": 6, "id": "ec06d3a7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/04/30 01:03:35 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"}, {"data": {"text/plain": "<pyspark.sql.streaming.StreamingQuery at 0x7fccbd81f130>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_5.writeStream.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n    .option(\"checkpointLocation\", \"gs://dataproc-staging-us-central1-398592550746-z8akpufv/pagerank_whole_05\") \\\n    .option(\"path\", \"gs://dataproc-staging-us-central1-398592550746-z8akpufv/pagerank_whole_05\").start()"}, {"cell_type": "code", "execution_count": 10, "id": "0162aeab", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\n\n\nwhole_path = 'gs://dataproc-staging-us-central1-398592550746-z8akpufv/pagerank_whole_05'\nspark = SparkSession.builder.getOrCreate()\ndt = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", False).csv(whole_path + \"/*.csv\")\n"}, {"cell_type": "code", "execution_count": 11, "id": "313d827f", "metadata": {}, "outputs": [{"data": {"text/plain": "1265848"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}, {"name": "stderr", "output_type": "stream", "text": "22/04/30 01:16:26 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2936 ms\n22/04/30 01:17:17 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2233 ms\n22/04/30 01:17:35 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 6220 ms\n22/04/30 01:17:48 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 21297 ms\n22/04/30 01:21:26 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2263 ms\n22/04/30 01:21:35 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2174 ms\n22/04/30 01:22:33 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 5233 ms\n22/04/30 01:22:59 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2253 ms\n22/04/30 01:23:09 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 8928 ms\n22/04/30 01:25:07 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 3962 ms\n22/04/30 01:26:12 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 7538 ms\n22/04/30 01:26:28 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 3569 ms\n22/04/30 01:27:14 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 3906 ms\n22/04/30 01:27:24 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 17 file(s) in 2087 ms\n"}], "source": "dt.count()"}, {"cell_type": "code", "execution_count": null, "id": "c3e50bff", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 5}