{"cells":[{"cell_type":"code","execution_count":1,"id":"60397dc7","metadata":{},"outputs":[],"source":["#set-up\n","import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'\n","\n","import regex\n","from pyspark.sql.functions import explode, udf, col, lower, when\n","from pyspark.sql.types import ArrayType, StringType"]},{"cell_type":"code","execution_count":2,"id":"08c81ff5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","com.databricks#spark-xml_2.12 added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-cc9a1e0f-38b3-4e6f-b9fa-44adc1bd29be;1.0\n","\tconfs: [default]\n","\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n","\tfound commons-io#commons-io;2.8.0 in central\n","\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n","\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n","downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.14.0/spark-xml_2.12-0.14.0.jar ...\n","\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.14.0!spark-xml_2.12.jar (36ms)\n","downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar ...\n","\t[SUCCESSFUL ] commons-io#commons-io;2.8.0!commons-io.jar (25ms)\n","downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/2.3.4/txw2-2.3.4.jar ...\n","\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;2.3.4!txw2.jar (83ms)\n","downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.2.5/xmlschema-core-2.2.5.jar ...\n","\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.2.5!xmlschema-core.jar(bundle) (20ms)\n",":: resolution report :: resolve 4417ms :: artifacts dl 177ms\n","\t:: modules in use:\n","\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n","\tcommons-io#commons-io;2.8.0 from central in [default]\n","\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n","\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-cc9a1e0f-38b3-4e6f-b9fa-44adc1bd29be\n","\tconfs: [default]\n","\t4 artifacts copied, 0 already retrieved (676kB/27ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/04/30 01:02:05 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/04/30 01:02:05 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/04/30 01:02:05 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/04/30 01:02:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/04/30 01:02:12 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.14.0.jar added multiple times to distributed cache.\n","22/04/30 01:02:12 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.8.0.jar added multiple times to distributed cache.\n","22/04/30 01:02:12 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-2.3.4.jar added multiple times to distributed cache.\n","22/04/30 01:02:12 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar added multiple times to distributed cache.\n"]}],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","execution_count":3,"id":"ef6d9780","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/04/30 01:02:24 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/04/30 01:02:24 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","                                                                                \r"]}],"source":["# load the data as a dataframe\n","df = spark.read.format('xml').options(rowTag='page').load('hdfs:/enwiki_small.xml')"]},{"cell_type":"code","execution_count":4,"id":"6133b247","metadata":{},"outputs":[],"source":["# we define a function that breaks down the text in our revision colunm into the links inside the brakets and apply \n","# the logic listed in the assignment: filtering out links containing ':' and '#' and only including the first link \n","#if multiple links are present. \n","\n","def return_links(text):\n","    try:\n","        matches = regex.findall(r'\\[\\[((?:[^[\\]]+|(?R))*+)\\]\\]', text)\n","    except:\n","        matches = []\n","    output = []\n","    for match in matches:\n","        for link in match.split('|'):\n","            if ':' in link and 'Category:' not in link:\n","                continue\n","            elif '#' in link:\n","                continue\n","            else:\n","                output.append(link.lower())\n","                break\n","    return output\n","\n","return_links_udf = udf(lambda text: return_links(text), ArrayType(StringType()))"]},{"cell_type":"code","execution_count":5,"id":"63e2685b","metadata":{},"outputs":[],"source":["#We select only the columns of interest from our dataframe. we only want the 'title' \n","#and the 'revision' columns saved as 'text'\n","new_test = df.select(col('title'), col('revision.text._VALUE').alias('text'))"]},{"cell_type":"code","execution_count":6,"id":"5c224534","metadata":{},"outputs":[],"source":["#Now we can apply our equation which returns all hyperlinks included between brackets and applies the logic necessary. \n","new_test = new_test.withColumn('inner_links', explode(return_links_udf(col('text'))))"]},{"cell_type":"code","execution_count":7,"id":"aaea112a","metadata":{},"outputs":[],"source":["#We take the results and ensure all results are lowercase, turning any uppercase leters to lowercase. \n","new_test = new_test.select(lower(col('title')).alias('new_title'), lower(col('inner_links')).alias('new_inner_links'))"]},{"cell_type":"code","execution_count":8,"id":"ec8c2441","metadata":{},"outputs":[],"source":["#Drop na fields\n","new_test = new_test.select(col('new_title'),col('new_inner_links')).na.drop()"]},{"cell_type":"code","execution_count":9,"id":"a6da4da5","metadata":{},"outputs":[],"source":["#sort in ascending order \n","new_test= new_test.select(col('new_title'),col('new_inner_links')).sort([\"new_title\",\"new_inner_links\"],ascending=True)"]},{"cell_type":"code","execution_count":12,"id":"0528f37d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#save the output as a cvs file\n","new_test.limit(5).write.option('delimeter', '\\t').csv('gs://adrianstorage/p1t2', mode = 'overwrite')"]},{"cell_type":"code","execution_count":13,"id":"1a9aa2ae","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["new_test.coalesce(1).write.option('delimeter', '\\t').csv('gs://adrianstorage/p1t2_whole', mode = 'overwrite')"]},{"cell_type":"code","execution_count":null,"id":"b6a067ef","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}